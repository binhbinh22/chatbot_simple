{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVS3u3PqwZSp",
        "outputId": "30f5d406-3d71-4f6d-b50d-1862fa45b0e1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "2Ggmx-t7vxGE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ec0bb7b-236b-4a44-94ef-7e639c16f3be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.optimizers import SGD\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import unicodedata\n",
        "\n",
        "def remove_diacritics_and_lower(input_str):\n",
        "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
        "    without_diacritics = ''.join([c for c in nfkd_form if not unicodedata.combining(c)])\n",
        "    return without_diacritics.lower()\n",
        "\n",
        "# Đọc nội dung từ tệp intents.json\n",
        "with open('/content/sample_data/intents.json', 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Loại bỏ dấu diacritics và chuyển đổi thành chữ thường từ mỗi chuỗi trong dữ liệu\n",
        "for intent in data['intents']:\n",
        "    for key in intent:\n",
        "        if isinstance(intent[key], list):\n",
        "            for i, item in enumerate(intent[key]):\n",
        "                intent[key][i] = remove_diacritics_and_lower(item)\n",
        "        else:\n",
        "            intent[key] = remove_diacritics_and_lower(intent[key])\n",
        "\n",
        "# Ghi lại nội dung đã thay đổi vào tệp\n",
        "with open('intents.json', 'w', encoding='utf-8') as file:\n",
        "    json.dump(data, file, ensure_ascii=False, indent=4)\n"
      ],
      "metadata": {
        "id": "a5CNC5yPtNwA"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "intents = json.loads(open('/content/intents.json').read())\n",
        "\n",
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_letters = ['?', '!',',','.']\n",
        "\n",
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        word_list = nltk.word_tokenize(pattern)\n",
        "        words.extend(word_list)\n",
        "        documents.append((word_list,intent['tag']))\n",
        "        if intent['tag'] not in classes:\n",
        "            classes.append(intent['tag'])\n",
        "\n",
        "words = [lemmatizer.lemmatize(word) for word in words if word not in ignore_letters]\n",
        "words = sorted(set(words))\n",
        "\n",
        "classes = sorted(set(classes))\n",
        "\n",
        "pickle.dump(words, open('words.pkl', 'wb'))\n",
        "pickle.dump(classes, open('classes.pkl', 'wb'))\n"
      ],
      "metadata": {
        "id": "dPFi5I6iv9qr"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training = []\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "for document in documents:\n",
        "    bag =[]\n",
        "    word_patterns = document[0]\n",
        "    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n",
        "    for word in words:\n",
        "        bag.append(1) if word in word_patterns else bag.append(0)\n",
        "\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(document[1])] = 1\n",
        "    if len(bag) > len(output_row):\n",
        "        output_row.extend([0] * (len(bag) - len(output_row)))\n",
        "    elif len(bag) < len(output_row):\n",
        "        bag.extend([0] * (len(output_row) - len(bag)))\n",
        "    training.append([bag, output_row])\n",
        "\n",
        "random.shuffle(training)\n",
        "# Check the lengths of 'bag' and 'output_row' lists\n",
        "print(len(bag))\n",
        "print(len(output_row))\n",
        "\n",
        "\n",
        "# Convert the 'training' list to a NumPy array\n",
        "training = np.array(training)\n",
        "training = np.array(training)\n",
        "\n",
        "train_x = list(training[:, 0])\n",
        "train_y = list(training[:, 1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MbALH8JjZ9r",
        "outputId": "3f913395-6c54-4422-9b0b-40dbf8087536"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "275\n",
            "275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
        "\n",
        "initial_learning_rate = 0.01\n",
        "lr_schedule = ExponentialDecay(\n",
        "    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n",
        ")\n",
        "\n",
        "# Use SGD with the learning rate schedule\n",
        "sgd = SGD(learning_rate=lr_schedule, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n",
        "model.save('chatbotmodel.h5', hist)\n",
        "\n",
        "print('Done')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xca1GqHswCqz",
        "outputId": "8023f6eb-d6ab-4cf6-c169-0b9d285d8bf4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "21/21 [==============================] - 1s 4ms/step - loss: 5.5930 - accuracy: 0.0000e+00\n",
            "Epoch 2/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 5.4920 - accuracy: 0.0485\n",
            "Epoch 3/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 5.3252 - accuracy: 0.1068\n",
            "Epoch 4/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 5.1276 - accuracy: 0.0388\n",
            "Epoch 5/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 4.6747 - accuracy: 0.1068\n",
            "Epoch 6/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 4.3374 - accuracy: 0.0583\n",
            "Epoch 7/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 3.9717 - accuracy: 0.0583\n",
            "Epoch 8/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 3.6380 - accuracy: 0.1068\n",
            "Epoch 9/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 3.7002 - accuracy: 0.0777\n",
            "Epoch 10/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 3.6827 - accuracy: 0.0874\n",
            "Epoch 11/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 3.3519 - accuracy: 0.1068\n",
            "Epoch 12/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 3.2210 - accuracy: 0.1456\n",
            "Epoch 13/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 3.0654 - accuracy: 0.1845\n",
            "Epoch 14/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 3.0123 - accuracy: 0.1845\n",
            "Epoch 15/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 2.8571 - accuracy: 0.2039\n",
            "Epoch 16/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 2.8096 - accuracy: 0.2233\n",
            "Epoch 17/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.6424 - accuracy: 0.3107\n",
            "Epoch 18/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.4886 - accuracy: 0.2524\n",
            "Epoch 19/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.3187 - accuracy: 0.3495\n",
            "Epoch 20/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.2031 - accuracy: 0.3107\n",
            "Epoch 21/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 2.2623 - accuracy: 0.3301\n",
            "Epoch 22/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.2005 - accuracy: 0.3981\n",
            "Epoch 23/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.1169 - accuracy: 0.3495\n",
            "Epoch 24/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 2.0746 - accuracy: 0.4369\n",
            "Epoch 25/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.7816 - accuracy: 0.4757\n",
            "Epoch 26/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.9278 - accuracy: 0.3883\n",
            "Epoch 27/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 1.7231 - accuracy: 0.4563\n",
            "Epoch 28/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 1.6144 - accuracy: 0.5049\n",
            "Epoch 29/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 1.6839 - accuracy: 0.5049\n",
            "Epoch 30/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.3221 - accuracy: 0.5922\n",
            "Epoch 31/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.5037 - accuracy: 0.5146\n",
            "Epoch 32/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.2982 - accuracy: 0.5922\n",
            "Epoch 33/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 1.3434 - accuracy: 0.5922\n",
            "Epoch 34/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.5712 - accuracy: 0.4757\n",
            "Epoch 35/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.4002 - accuracy: 0.5437\n",
            "Epoch 36/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 1.3906 - accuracy: 0.5922\n",
            "Epoch 37/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.2599 - accuracy: 0.5922\n",
            "Epoch 38/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 1.3279 - accuracy: 0.6214\n",
            "Epoch 39/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 1.2009 - accuracy: 0.6117\n",
            "Epoch 40/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.0154 - accuracy: 0.6505\n",
            "Epoch 41/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 1.0154 - accuracy: 0.7184\n",
            "Epoch 42/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 1.1237 - accuracy: 0.6699\n",
            "Epoch 43/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.9546 - accuracy: 0.7670\n",
            "Epoch 44/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.8894 - accuracy: 0.7282\n",
            "Epoch 45/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.9412 - accuracy: 0.7282\n",
            "Epoch 46/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.9424 - accuracy: 0.7476\n",
            "Epoch 47/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.0416 - accuracy: 0.6796\n",
            "Epoch 48/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.7546 - accuracy: 0.7864\n",
            "Epoch 49/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 1.0582 - accuracy: 0.6699\n",
            "Epoch 50/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.8519 - accuracy: 0.7282\n",
            "Epoch 51/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.8031 - accuracy: 0.7379\n",
            "Epoch 52/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.9808 - accuracy: 0.6893\n",
            "Epoch 53/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 1.0202 - accuracy: 0.6699\n",
            "Epoch 54/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.7834 - accuracy: 0.7864\n",
            "Epoch 55/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.8840 - accuracy: 0.7184\n",
            "Epoch 56/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.6376 - accuracy: 0.7670\n",
            "Epoch 57/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.7336 - accuracy: 0.7767\n",
            "Epoch 58/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.6372 - accuracy: 0.7961\n",
            "Epoch 59/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.5833 - accuracy: 0.8155\n",
            "Epoch 60/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.6863 - accuracy: 0.7864\n",
            "Epoch 61/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.7744 - accuracy: 0.7670\n",
            "Epoch 62/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.7346 - accuracy: 0.7670\n",
            "Epoch 63/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.6411 - accuracy: 0.7767\n",
            "Epoch 64/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.7785 - accuracy: 0.6699\n",
            "Epoch 65/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.6259 - accuracy: 0.7961\n",
            "Epoch 66/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.7585 - accuracy: 0.7767\n",
            "Epoch 67/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.4975 - accuracy: 0.8350\n",
            "Epoch 68/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.5439 - accuracy: 0.8447\n",
            "Epoch 69/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.5689 - accuracy: 0.8058\n",
            "Epoch 70/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.6160 - accuracy: 0.8155\n",
            "Epoch 71/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.5587 - accuracy: 0.8155\n",
            "Epoch 72/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.5888 - accuracy: 0.7864\n",
            "Epoch 73/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.5583 - accuracy: 0.8350\n",
            "Epoch 74/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.4678 - accuracy: 0.8447\n",
            "Epoch 75/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 0.4348 - accuracy: 0.8738\n",
            "Epoch 76/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 0.4137 - accuracy: 0.8641\n",
            "Epoch 77/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.4900 - accuracy: 0.8544\n",
            "Epoch 78/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.4917 - accuracy: 0.8641\n",
            "Epoch 79/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.4233 - accuracy: 0.8738\n",
            "Epoch 80/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.4752 - accuracy: 0.7961\n",
            "Epoch 81/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.5366 - accuracy: 0.8350\n",
            "Epoch 82/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.4235 - accuracy: 0.8738\n",
            "Epoch 83/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.4039 - accuracy: 0.8447\n",
            "Epoch 84/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.4805 - accuracy: 0.8350\n",
            "Epoch 85/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.4721 - accuracy: 0.8447\n",
            "Epoch 86/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.3801 - accuracy: 0.8738\n",
            "Epoch 87/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.5148 - accuracy: 0.8350\n",
            "Epoch 88/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.4698 - accuracy: 0.8544\n",
            "Epoch 89/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.5412 - accuracy: 0.8641\n",
            "Epoch 90/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.3534 - accuracy: 0.9029\n",
            "Epoch 91/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.4272 - accuracy: 0.8738\n",
            "Epoch 92/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.4091 - accuracy: 0.8738\n",
            "Epoch 93/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.5511 - accuracy: 0.8447\n",
            "Epoch 94/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.3178 - accuracy: 0.9029\n",
            "Epoch 95/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.3710 - accuracy: 0.8932\n",
            "Epoch 96/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.3821 - accuracy: 0.8932\n",
            "Epoch 97/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.3265 - accuracy: 0.9029\n",
            "Epoch 98/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.4091 - accuracy: 0.8447\n",
            "Epoch 99/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.4310 - accuracy: 0.8447\n",
            "Epoch 100/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.3044 - accuracy: 0.8932\n",
            "Epoch 101/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.5416 - accuracy: 0.8252\n",
            "Epoch 102/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.4785 - accuracy: 0.8058\n",
            "Epoch 103/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.3409 - accuracy: 0.9126\n",
            "Epoch 104/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.3751 - accuracy: 0.8738\n",
            "Epoch 105/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2750 - accuracy: 0.9126\n",
            "Epoch 106/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.3660 - accuracy: 0.8738\n",
            "Epoch 107/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.3288 - accuracy: 0.9223\n",
            "Epoch 108/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.3588 - accuracy: 0.8835\n",
            "Epoch 109/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.3058 - accuracy: 0.9126\n",
            "Epoch 110/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.3163 - accuracy: 0.8932\n",
            "Epoch 111/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.4288 - accuracy: 0.8350\n",
            "Epoch 112/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.4131 - accuracy: 0.8350\n",
            "Epoch 113/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.4229 - accuracy: 0.8350\n",
            "Epoch 114/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.4106 - accuracy: 0.8835\n",
            "Epoch 115/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.4150 - accuracy: 0.8835\n",
            "Epoch 116/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.3953 - accuracy: 0.8738\n",
            "Epoch 117/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.2152 - accuracy: 0.9417\n",
            "Epoch 118/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.4207 - accuracy: 0.9126\n",
            "Epoch 119/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2171 - accuracy: 0.9223\n",
            "Epoch 120/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.3672 - accuracy: 0.8641\n",
            "Epoch 121/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2937 - accuracy: 0.9417\n",
            "Epoch 122/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.1802 - accuracy: 0.9709\n",
            "Epoch 123/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2863 - accuracy: 0.8932\n",
            "Epoch 124/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.2847 - accuracy: 0.9417\n",
            "Epoch 125/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.3788 - accuracy: 0.8835\n",
            "Epoch 126/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.2644 - accuracy: 0.9126\n",
            "Epoch 127/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2821 - accuracy: 0.8932\n",
            "Epoch 128/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.3558 - accuracy: 0.9029\n",
            "Epoch 129/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2993 - accuracy: 0.8544\n",
            "Epoch 130/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.2616 - accuracy: 0.9029\n",
            "Epoch 131/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.2908 - accuracy: 0.9126\n",
            "Epoch 132/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.3385 - accuracy: 0.9223\n",
            "Epoch 133/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.3891 - accuracy: 0.8350\n",
            "Epoch 134/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.3740 - accuracy: 0.8932\n",
            "Epoch 135/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2863 - accuracy: 0.9126\n",
            "Epoch 136/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2167 - accuracy: 0.9029\n",
            "Epoch 137/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.3809 - accuracy: 0.8544\n",
            "Epoch 138/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2000 - accuracy: 0.9515\n",
            "Epoch 139/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.1861 - accuracy: 0.9320\n",
            "Epoch 140/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.1697 - accuracy: 0.9417\n",
            "Epoch 141/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.2555 - accuracy: 0.9223\n",
            "Epoch 142/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.3528 - accuracy: 0.8932\n",
            "Epoch 143/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.1737 - accuracy: 0.9709\n",
            "Epoch 144/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2549 - accuracy: 0.9223\n",
            "Epoch 145/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2567 - accuracy: 0.9126\n",
            "Epoch 146/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.1435 - accuracy: 0.9612\n",
            "Epoch 147/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2828 - accuracy: 0.9029\n",
            "Epoch 148/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.3717 - accuracy: 0.8738\n",
            "Epoch 149/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2225 - accuracy: 0.9320\n",
            "Epoch 150/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2514 - accuracy: 0.9029\n",
            "Epoch 151/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2481 - accuracy: 0.9223\n",
            "Epoch 152/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2950 - accuracy: 0.8835\n",
            "Epoch 153/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2769 - accuracy: 0.9126\n",
            "Epoch 154/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.1848 - accuracy: 0.9612\n",
            "Epoch 155/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.3190 - accuracy: 0.9126\n",
            "Epoch 156/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.1601 - accuracy: 0.9515\n",
            "Epoch 157/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2982 - accuracy: 0.9029\n",
            "Epoch 158/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.3137 - accuracy: 0.8835\n",
            "Epoch 159/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.1881 - accuracy: 0.9320\n",
            "Epoch 160/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2316 - accuracy: 0.9223\n",
            "Epoch 161/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2023 - accuracy: 0.9417\n",
            "Epoch 162/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2765 - accuracy: 0.8932\n",
            "Epoch 163/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2417 - accuracy: 0.9515\n",
            "Epoch 164/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2505 - accuracy: 0.9223\n",
            "Epoch 165/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2392 - accuracy: 0.9126\n",
            "Epoch 166/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.2365 - accuracy: 0.8835\n",
            "Epoch 167/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2943 - accuracy: 0.9126\n",
            "Epoch 168/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.1962 - accuracy: 0.9223\n",
            "Epoch 169/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2633 - accuracy: 0.9417\n",
            "Epoch 170/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.3230 - accuracy: 0.9126\n",
            "Epoch 171/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.1424 - accuracy: 0.9515\n",
            "Epoch 172/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.1617 - accuracy: 0.9515\n",
            "Epoch 173/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.2280 - accuracy: 0.9223\n",
            "Epoch 174/200\n",
            "21/21 [==============================] - 0s 3ms/step - loss: 0.1873 - accuracy: 0.9320\n",
            "Epoch 175/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.3659 - accuracy: 0.8447\n",
            "Epoch 176/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2738 - accuracy: 0.8835\n",
            "Epoch 177/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.3330 - accuracy: 0.9029\n",
            "Epoch 178/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2498 - accuracy: 0.9515\n",
            "Epoch 179/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 0.2680 - accuracy: 0.8835\n",
            "Epoch 180/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.1909 - accuracy: 0.9515\n",
            "Epoch 181/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2219 - accuracy: 0.9029\n",
            "Epoch 182/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2354 - accuracy: 0.9223\n",
            "Epoch 183/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2943 - accuracy: 0.9029\n",
            "Epoch 184/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2111 - accuracy: 0.9126\n",
            "Epoch 185/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.1397 - accuracy: 0.9515\n",
            "Epoch 186/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.1379 - accuracy: 0.9515\n",
            "Epoch 187/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2354 - accuracy: 0.9515\n",
            "Epoch 188/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.1157 - accuracy: 0.9709\n",
            "Epoch 189/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2203 - accuracy: 0.9320\n",
            "Epoch 190/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.1731 - accuracy: 0.9417\n",
            "Epoch 191/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2234 - accuracy: 0.9515\n",
            "Epoch 192/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2189 - accuracy: 0.9029\n",
            "Epoch 193/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.1524 - accuracy: 0.9612\n",
            "Epoch 194/200\n",
            "21/21 [==============================] - 0s 6ms/step - loss: 0.2868 - accuracy: 0.9126\n",
            "Epoch 195/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2297 - accuracy: 0.9223\n",
            "Epoch 196/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.3172 - accuracy: 0.8932\n",
            "Epoch 197/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.2501 - accuracy: 0.9126\n",
            "Epoch 198/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.2857 - accuracy: 0.9320\n",
            "Epoch 199/200\n",
            "21/21 [==============================] - 0s 5ms/step - loss: 0.2817 - accuracy: 0.8835\n",
            "Epoch 200/200\n",
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2652 - accuracy: 0.9223\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "hV1iaOHY4XFZ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "intents = json.loads(open('/content/intents.json').read())\n",
        "words = pickle.load(open('/content/words.pkl', 'rb'))\n",
        "classes = pickle.load(open('/content/classes.pkl', 'rb'))\n",
        "model = load_model('/content/chatbotmodel.h5')\n"
      ],
      "metadata": {
        "id": "Iq26AXuz4cKo"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_up_sentence(sentence):\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "    sentence_words = [lemmatizer.lemmatize(word)  for word in sentence_words]\n",
        "    return sentence_words\n",
        "\n",
        "def bag_of_words(sentence):\n",
        "    sentence_words= clean_up_sentence(sentence)\n",
        "    bag = [0] * len(words)\n",
        "    for w in sentence_words:\n",
        "        for i, word in enumerate(words):\n",
        "            if word == w:\n",
        "                bag[i] = 1\n",
        "\n",
        "    return np.array(bag)\n",
        "\n",
        "def predict_class(sentence):\n",
        "    bow = bag_of_words(sentence)\n",
        "    res = model.predict(np.array([bow]))[0]\n",
        "    ERROR_THRESHOLD = 0.25\n",
        "    results = [[i,r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]\n",
        "\n",
        "    results.sort(key=lambda  x:x[1], reverse=True)\n",
        "    return_list = []\n",
        "    for r in results:\n",
        "        return_list.append({'intent': classes[r[0]], 'probability': str(r[1])})\n",
        "    return return_list\n",
        "\n",
        "\n",
        "def get_response(intents_list,intents_json):\n",
        "    tag= intents_list[0]['intent']\n",
        "    list_of_intents =intents_json['intents']\n",
        "    for i in list_of_intents:\n",
        "        if i['tag'] == tag:\n",
        "            result = random.choice(i['responses'])\n",
        "            break\n",
        "    return result"
      ],
      "metadata": {
        "id": "WIfkyaHX4oFP"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    message = input(\"| You: \")\n",
        "    if message == \"bye\" or message == \"Goodbye\":\n",
        "        ints = predict_class(message)\n",
        "        res = get_response(ints, intents)\n",
        "        print(\"| Bot:\", res)\n",
        "        print(\"|===================== The Program End here! =====================|\")\n",
        "        exit()\n",
        "\n",
        "    else:\n",
        "        ints = predict_class(message)\n",
        "        res = get_response(ints, intents)\n",
        "        print(\"| Bot:\", res)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aQMOH3Kl4p47",
        "outputId": "166aa9f1-89be-4c54-f0a6-1b6dbfc1135c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| You: xin chào\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "| Bot: xin chao, rat vui duoc gap lai ban\n",
            "| You: ban bao nhieu tuoi \n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "| Bot: hai muoi mot tuoi\n",
            "| You: thu ba 26/3/2024 co su kien gi\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "| Bot: tu toi 25/3, bitcoin kich hoat đa tang sau thoi gian dai giao dich quanh vung 64.000-68.000 usd mot đon vi. khoang 23h45, tien so lon nhat the gioi vuot 70.000 usd, tang hon 7% trong 24 gio.\n",
            "thi gia co chieu huong đi len, đen sang 26/3, bitcoin co luc đat hon 71.162 usd. trong buoi sang, thi truong co rung lac nhung tien so nay van giu đuoc muc cao nhat 10 ngay qua. luc 9h30, moi đong bitcoin khoang 70.238 usd, cao hon nguong đinh thiet lap thang 11/2021 (68.818 usd).\n",
            "cac tien so khac cung dien bien tich cuc. ether đa tang 6%, solana va avalanche cung tich luy hon 10% trong 24 gio.\n",
            "viec bitcoin lay lai moc 70.000 usd bao hieu đot đieu chinh co the ket thuc. ve đa tang gia lan nay, cong ty phan tich 10x research cho biet nha đau tu đang ky vong bitcoin co the nham toi muc cao moi voi nhieu du kien kha quan. theo phan tich ky thuat, đo thi gia cua đong tien nay đang co dau hieu chuan bi nhay them 15.000-20.000 usd cho đot van đong tiep theo. cu nhay nay se đay bitcoin len toi 83.000 usd mot đon vi.\n",
            "\n",
            "theo markus thielen - nha sang lap 10x research, viec tien so lay lai muc đinh nam 2021 la mot du kien tot. \"khi cac đinh cua chu ky truoc đo đuoc kiem tra lai va pha vo mot lan nua, bitcoin co xu huong tao ra cac đot tang gia đang ke\", chuyen gia nay noi.\n",
            "bao cao cua 10x research cho biet, xu huong tang gia cung đuoc ho tro boi mot so ngan hang trung uong nghieng ve quan điem on hoa. cuc du tru lien bang my (fed) phat đi tin hieu rang ho san sang chap nhan lam phat cao hon trong thoi gian dai hon va mong muon giam toc đo that chat tien te. ngan hang nhat ban va ngan hang thuy si cung đua ra cac binh luan on hoa.\n",
            "ngoai ra, bitcoin co xu huong hoat đong tot trong nhung nam bau cu o my, tang truong 100-200% trong lich su. \"ky vong tang gia cua chung toi la 83.000 usd va 102.000 usd co the dan dan đuoc thuc hien\", markus thielen du đoan.\n",
            "truoc đo, sau khi lap ky luc hon 73.750 usd ngay 14/3, tien so lon nhat the gioi bi ban thao roi nhanh chong giam xuong duoi 61.000 usd. adam sze - nguoi đung đau tai san ky thuat so cua cong ty đau tu global x cho rang bat cu khi nao mot tai san đat muc cao nhat moi thoi đai đeu co xu huong chot loi hoac ghi nhan toc đo mua cham lai.\n",
            "đot tang gia cua tien so truoc đo đuoc thuc đay boi dong von đo vao cac quy etf giao ngay tai my. tuan truoc, dong tien đo vao cac quy nay cham lai đang ke do dien bien thi truong. trong đo, grayscale bitcoin trust ghi nhan dong tien chay ra 1,9 ty usd khien tong von cua cac quy etf bitcoin roi vao tinh trang bi rut rong.\n",
            "tieu gu (theo coindesk, reuters)\n",
            "| You: thu hai 25/3/2024 co su kien gi \n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "| Bot: tai phien hop thuong ky đau thang 3, thu tuong yeu cau cac doanh nghiep ban le xang dau phai xuat hoa đon đien tu tung lan va ket noi du lieu voi co quan thue ngay trong thang nay, keo dai 3 thang so voi chi đao cuoi nam ngoai.\n",
            "theo yeu cau cua thu tuong, cac doanh nghiep khong thuc hien đung han lan nay se bi xu ly, che tai nang nhat la tam dung hoat đong, thu hoi giay phep, chung nhan đu đieu kien kinh doanh.\n",
            "đen ngay 24/3, thong ke cua tong cuc thue cho thay 14.700 cay xang, chiem khoang 92% va tang khoang 5,6 lan so voi thoi điem ban đau trien khai. nhu vay, chi con hon 1.250 cua hang chua thuc hien, chiem 7,8%.\n",
            "trong đo, nghe an, thai nguyen, quang tri, binh duong, ha noi, ba ria - vung tau, đak lak, an giang, ben tre, bac ninh, ha nam, đien bien, ninh binh, tra vinh la cac đia phuong đa hoan thanh 100%. chi co 5 đia phuong gom bac kan, kon tum, kien giang, lam đong, cao bang co tien đo duoi 70%.\n",
            "theo quy đinh cua luat quan ly thue, co hieu luc tu thang 7/2022, cac cua hang ban le xang dau tren ca nuoc phai xuat hoa đon đien tu tung lan ban va ket noi du lieu voi co quan thue.\n",
            "cuoi nam ngoai, thu tuong chi đao bo tai chinh co giai phap đe cac cua hang xang dau thuc hien viec nay trong thang 12/2023. nhung toi luc đo, chi 2.700 cua hang trien khai, chu yeu cua petrolimex va saigon petro.\n",
            "theo tong cuc thue, viec cac doanh nghiep xang dau xuat hoa đon đien tu tung lan se giup co quan quan ly kiem soat viec phat hanh hoa đon ban le, ngan chan gian lan, han che buon lau xang dau. viec nay cung đam bao moi truong kinh doanh cong bang, minh bach voi cac nganh nghe khac, tang thu ngan sach nha nuoc.\n",
            "trong qua trinh thuc hien, mot so doanh nghiep phan anh gap vuong lien quan toi cac van đe ky thuat nhu lua chon cong nghe, kiem đinh cot bom. ngoai ra, chi phi 40-60 đong mot hoa đon khi xuat ra cung la ap luc tai chinh cho doanh nghiep khi trien khai. chua ke, doanh nghiep lo phai chiu cac chi phi đuong truyen ket noi, chi phi thanh toan neu nguoi tieu dung su dung the, chi phi phan mem ban hang, quan ly xang dau...\n",
            "tuy nhien, mot so doanh nghiep đau moi cho rang trien khai hoa đon đien tu tung lan giup ho đay nhanh đoi moi quan tri, kiem soat, kiem tra du lieu minh bach, nhanh chong. voi khach hang khong kinh doanh, khong co nhu cau lay hoa đon đien tu, he thong cua doanh nghiep tu đong xu ly, phat hanh, luu tru hoa đon duoi hinh thuc đien tu va truyen ve co quan thue theo bang tong hop du lieu tung mat hang ban trong ngay.\n",
            "cung voi xang dau, hau het cua hang kinh doanh dich vu an uong, cua hang tap hoa, sieu thi hay tiem thuoc tan duoc... đeu đa xuat hoa đon đien tu tung lan. viec nay giup khach hang co the kiem soat đuoc hang hoa đa mua.\n",
            "phuong dung\n",
            "| You: gia bo kobe hien nay the nao\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "| Bot: hn\n",
            "| You: bitcoin hien tai the nao\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "| Bot: hn\n",
            "| You: bitcoin thu ba 25/3\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "| Bot: chia se tai toa đam \"canh bao su dung đien mua nang nong\" chieu 19/3 do bao tuoi tre to chuc, ong pham thanh tung, lanh đao cong ty tnhh lavie lo lang thieu đien se anh huong toi hoat đong kinh doanh cua doanh nghiep.\n",
            "nam ngoai, thieu đien xay ra tai mien bac trong nua cuoi thang 5, đau thang 6 đa đe lai nhieu anh huong trong san xuat, kinh doanh cua doanh nghiep. mot so khu cong nghiep thoi điem đo cat đien co bao truoc, voi tan suat 1-2 lan mot tuan.\n",
            "ong tung đe nghi evn nen co du bao som đe doanh nghiep co phuong an ung pho, tranh tai dien thieu đien nhu mua nong 2023.\n",
            "truoc lo lang cua doanh nghiep, ong vo quang lam, pho tong giam đoc tap đoan đien luc viet nam (evn), noi tap đoan nay se tang huy đong cac nguon đien gia cao va nang luong tai tao đe \"nam nay khong thieu đien\".\n",
            "thuy đien, nhiet đien va nang luong tai tao la ba nguon đien chinh cua he thong đien quoc gia. theo so lieu cua evn, huy đong cac nguon nay trong thang 1 chiem khoang 90% san luong đien toan he thong. trong đo, đien than la 12,7 ty kwh, tuong đuong 53%.\n",
            "ong lam cho biet, nam nay evn se tang mua tu nhiet đien khoang 145% so voi 2023. \"chung toi phai huy đong nguon đien đat tien la nhiet đien vao đau mua kho, đe đam bao cung ung cho sinh hoat, san xuat\", pho tong evn noi.\n",
            "ngoai ra, tap đoan nay cung huy đong toi đa nguon nang luong tai tao. theo đo, san luong mua tang tu đien gio se 25% va đien mat troi 19% so voi nam ngoai.\n",
            "du lieu cua evn cho thay, nam ngoai gia mua nhiet đien khoang 2.100 đong mot kwh, cao hon gan 40% so voi thuy đien. muc nay tuong đuong gia tu cac nha may đien mat troi (9,35 cent mot kwh).\n",
            "ong nguyen the huu, pho cuc truong cuc đieu tiet đien luc (bo cong thuong), cho hay co quan nay cung evn đua ra nhieu phuong an, gom kich ban cung ung khi nhu cau tieu dung đien o muc cao. nganh đien cung đay nhanh xay dung cac tuyen truyen tai đien, đuong day 500 kv mach 3 keo đien ra mien bac.\n",
            "ngoai huy đong toi đa cac nguon trong nuoc, bo cong thuong, evn keu goi doanh nghiep va nguoi dan tiet kiem đien.\n",
            "tp hcm va nhieu đia phuong o phia nam đang vao cao điem nang nong. theo thong ke cua evn, hai thang đau nam, luong đien tieu thu tai thanh pho binh quan tren 75,3 trieu kwh mot ngay, tang gan 11,4% so voi cung ky nam ngoai. tuong tu, đien tieu thu cua 21 tinh, thanh phia nam cung cao hon 14% so voi cung ky nam 2023.\n",
            "ong bui trung kien, pho tong giam đoc tong cong ty đien luc tp hcm (evnhcm) du bao tieu thu đien cua cac ho gia đinh, doanh nghiep tai đay tang cao khi nen nhiet binh quan thang 3 khu vuc nay la 36,5 đo c. do đo, tien đien cua cac ho khi dung tren 401 kwh, co the tang 30-40% trong thang 3, va cao hon vao thang 4, 5.\n",
            "thi ha\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-fdd14bced892>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"| You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"bye\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Goodbye\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HBWZvT2JtMX9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}